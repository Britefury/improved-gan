{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import theano as th\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "import lasagne\n",
    "import lasagne.layers as ll\n",
    "from lasagne.init import Normal\n",
    "from lasagne.layers import dnn\n",
    "import nn\n",
    "import sys\n",
    "import plotting\n",
    "import cifar10_data\n",
    "import improved_gan\n",
    "from britefury_lasagne.dataset import balanced_subset_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "SEED = 1\n",
    "BATCH_SIZE = 100\n",
    "UNLABELED_WEIGHT = 1.0\n",
    "LEARNING_RATE = 0.0003\n",
    "DATA_DIR = './data/cifar10'\n",
    "N_LABELED = 4000 # Original code had COUNT=400, but thats *per class*, so 4000 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fixed random seeds\n",
    "rng = np.random.RandomState(SEED)\n",
    "lasagne.random.set_rng(np.random.RandomState(rng.randint(2 ** 15)))\n",
    "\n",
    "print('Loading data...')\n",
    "# load CIFAR-10\n",
    "trainx_all, trainy_all = cifar10_data.load(DATA_DIR, subset='train')\n",
    "testx, testy = cifar10_data.load(DATA_DIR, subset='test')\n",
    "\n",
    "# select labeled data\n",
    "train_indices = balanced_subset_indices(trainy_all, n_classes=10, n_samples=N_LABELED, shuffle=True, rng=rng)\n",
    "trainx = trainx_all[train_indices]\n",
    "trainy = trainy_all[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CIFAR10ImprovedGANSemiSupervisedClassifier (improved_gan.ImprovedGANSemiSupervisedClassifier):\n",
    "    def generative_model(self, noise_size, noise_expr):\n",
    "        # specify generative model\n",
    "        input_layer = ll.InputLayer(shape=(None, noise_size), input_var=noise_expr)\n",
    "        layers = [input_layer]\n",
    "        layers.append(nn.batch_norm(ll.DenseLayer(layers[-1], num_units=4*4*512, W=Normal(0.05), nonlinearity=nn.relu), g=None))\n",
    "        layers.append(ll.ReshapeLayer(layers[-1], ([0],512,4,4)))\n",
    "        layers.append(nn.batch_norm(nn.Deconv2DLayer(layers[-1], (None,256,8,8), (5,5), W=Normal(0.05), nonlinearity=nn.relu), g=None)) # 4 -> 8\n",
    "        layers.append(nn.batch_norm(nn.Deconv2DLayer(layers[-1], (None,128,16,16), (5,5), W=Normal(0.05), nonlinearity=nn.relu), g=None)) # 8 -> 16\n",
    "        layers.append(nn.weight_norm(nn.Deconv2DLayer(layers[-1], (None,3,32,32), (5,5), W=Normal(0.05), nonlinearity=T.tanh), train_g=True, init_stdv=0.1)) # 16 -> 32\n",
    "        return improved_gan.GenerativeModel(layers=layers, input_layer=input_layer, final_layer=layers[-1])\n",
    "\n",
    "\n",
    "    def discriminative_model(self):\n",
    "        # specify discriminative model\n",
    "        input_layer = ll.InputLayer(shape=(None, 3, 32, 32))\n",
    "        layers = [input_layer]\n",
    "        layers.append(ll.DropoutLayer(layers[-1], p=0.2))\n",
    "        layers.append(nn.weight_norm(dnn.Conv2DDNNLayer(layers[-1], 96, (3,3), pad=1, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(nn.weight_norm(dnn.Conv2DDNNLayer(layers[-1], 96, (3,3), pad=1, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(nn.weight_norm(dnn.Conv2DDNNLayer(layers[-1], 96, (3,3), pad=1, stride=2, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(ll.DropoutLayer(layers[-1], p=0.5))\n",
    "        layers.append(nn.weight_norm(dnn.Conv2DDNNLayer(layers[-1], 192, (3,3), pad=1, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(nn.weight_norm(dnn.Conv2DDNNLayer(layers[-1], 192, (3,3), pad=1, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(nn.weight_norm(dnn.Conv2DDNNLayer(layers[-1], 192, (3,3), pad=1, stride=2, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(ll.DropoutLayer(layers[-1], p=0.5))\n",
    "        layers.append(nn.weight_norm(dnn.Conv2DDNNLayer(layers[-1], 192, (3,3), pad=0, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(nn.weight_norm(ll.NINLayer(layers[-1], num_units=192, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        layers.append(nn.weight_norm(ll.NINLayer(layers[-1], num_units=192, W=Normal(0.05), nonlinearity=nn.lrelu)))\n",
    "        feature_layer = ll.GlobalPoolLayer(layers[-1])\n",
    "        layers.append(feature_layer)\n",
    "        return improved_gan.DiscriminativeModel(layers=layers, input_layer=input_layer, final_layer=layers[-1], feature_layer=feature_layer)\n",
    "\n",
    "\n",
    "    def on_batch_complete(self, epoch):\n",
    "        # generate samples from the model\n",
    "        z = self.rng.uniform(size=(100, self.noise_size)).astype(np.float32)\n",
    "        sample_x = self.samplefun(z)\n",
    "        img_bhwc = np.transpose(sample_x[:100,], (0, 2, 3, 1))\n",
    "        img_tile = plotting.img_tile(img_bhwc, aspect_ratio=1.0, border_color=1.0, stretch=True)\n",
    "        plotting.plot_img(img_tile, title='CIFAR10 samples', figsize=(8,8))\n",
    "        plotting.plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Building...')\n",
    "model = CIFAR10ImprovedGANSemiSupervisedClassifier(\n",
    "    improved_gan.FeatureMatchingStabilizer(),\n",
    "    n_classes=10, noise_size=100, unlabeled_weight=UNLABELED_WEIGHT, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Initialising...')\n",
    "model.initialise(trainx_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "learning_rate_fn = lambda epoch: LEARNING_RATE * np.minimum(3. - epoch/400., 1.)\n",
    "model.train(trainx, trainy, trainx_all, None, [testx, testy], num_epochs=1200,\n",
    "            batch_size=BATCH_SIZE, learning_rate=learning_rate_fn)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
